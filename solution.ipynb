{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70845b22",
   "metadata": {},
   "source": [
    "# Подготовка данных \n",
    "Очищаем и Делим на 80/10/10. К сожалению придется убрать полученные выборки, из-за большого объема. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Читаем сырые данные: data/raw_dataset.txt\n",
      "Очистка текстов...\n",
      "Очищенный датасет сохранен в data\\dataset_processed.csv\n",
      "Разбиение на выборки...\n",
      "Все файлы (train/val/test) успешно созданы в папке data/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./src') # Говорим Python: \"загляни в папку src\n",
    "\n",
    "from data_utils import prepare_data\n",
    "\n",
    "# Запускаем обработку\n",
    "# Если твой файл называется по-другому, просто замени имя здесь\n",
    "prepare_data('data/raw_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1c5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkote\\sprint1\\text-autocomplete\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Используем токенизатор от GPT-2, он отлично работает с текстом\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# У GPT-2 нет стандартного токена отступа, назначим его сами\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb1a60f",
   "metadata": {},
   "source": [
    "# Обучение\n",
    "Ниже закомментирована ячейка для обучения на GPU из-за проблем с ВМ. Обучаем быстро для части данных на CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0301536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from next_token_dataset import NextTokenDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Создаем объекты Dataset\n",
    "train_ds = NextTokenDataset('data/train.csv', tokenizer, max_length=64)\n",
    "val_ds = NextTokenDataset('data/val.csv', tokenizer, max_length=64)\n",
    "\n",
    "# Создаем DataLoader (те самые \"порции\" данных для обучения)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Данные готовы! Количество батчей в обучении: {len(train_loader)}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283a17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('./src') # Говорим Python: \"загляни в папку src\n",
    "from next_token_dataset import NextTokenDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Берем 10к строк - это хороший баланс между качеством и скоростью CPU\n",
    "train_df_10k = pd.read_csv('data/train.csv').sample(n=10000, random_state=42)\n",
    "val_df_small = pd.read_csv('data/val.csv').sample(n=1000, random_state=42)\n",
    "\n",
    "train_df_10k.to_csv('data/train_10k.csv', index=False)\n",
    "val_df_small.to_csv('data/val_10k.csv', index=False)\n",
    "\n",
    "train_ds = NextTokenDataset('data/train_10k.csv', tokenizer, max_length=64)\n",
    "val_ds = NextTokenDataset('data/val_10k.csv', tokenizer, max_length=64)\n",
    "\n",
    "# Batch size 64 на CPU обычно работает быстрее всего за счет векторизации\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08aed412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Берем одну порцию данных\\nx, y = next(iter(train_loader))\\n\\nprint(f\"Формат входа (Batch, Seq_len): {x.shape}\") \\nprint(f\"Пример первого токена во входе: {x[0][0]}\")\\nprint(f\"Пример первого токена в ответе: {y[0][0]}\") \\n# y[0][0] должен быть равен x[0][1], так как это \"следующий\" токен\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Берем одну порцию данных\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "print(f\"Формат входа (Batch, Seq_len): {x.shape}\") \n",
    "print(f\"Пример первого токена во входе: {x[0][0]}\")\n",
    "print(f\"Пример первого токена в ответе: {y[0][0]}\") \n",
    "# y[0][0] должен быть равен x[0][1], так как это \"следующий\" токен\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9dcbac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15737ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 157/157 [24:31<00:00,  9.37s/it, loss=7.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 Results ---\n",
      "Loss: 7.8803\n",
      "ROUGE-1: 0.5942 | ROUGE-2: 0.5642\n",
      "Prompt: 'i am going to' -> Gen: 'i am going to tovisjoy love'\n",
      "Prompt: 'today is a' -> Gen: 'today is a col out you stressful makes'\n",
      "Prompt: 'this movie was' -> Gen: 'this movie was you betweenletesgh from'\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 157/157 [21:23<00:00,  8.17s/it, loss=7.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 Results ---\n",
      "Loss: 6.8585\n",
      "ROUGE-1: 0.5876 | ROUGE-2: 0.5579\n",
      "Prompt: 'i am going to' -> Gen: 'i am going to be thunderot and anth'\n",
      "Prompt: 'today is a' -> Gen: 'today is afe what thing wanna r'\n",
      "Prompt: 'this movie was' -> Gen: 'this movie was ok new th certain well'\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 157/157 [20:12<00:00,  7.72s/it, loss=6.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 3 Results ---\n",
      "Loss: 6.5233\n",
      "ROUGE-1: 0.5938 | ROUGE-2: 0.5641\n",
      "Prompt: 'i am going to' -> Gen: 'i am going to fall for myzz one'\n",
      "Prompt: 'today is a' -> Gen: 'today is a great of aicky in'\n",
      "Prompt: 'this movie was' -> Gen: 'this movie was new gl diversity and haha'\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 157/157 [21:18<00:00,  8.14s/it, loss=6.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 4 Results ---\n",
      "Loss: 6.2429\n",
      "ROUGE-1: 0.5863 | ROUGE-2: 0.5569\n",
      "Prompt: 'i am going to' -> Gen: 'i am going to be our straight da one'\n",
      "Prompt: 'today is a' -> Gen: 'today is a response deserve to love a'\n",
      "Prompt: 'this movie was' -> Gen: 'this movie was good people today boring she'\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 157/157 [21:30<00:00,  8.22s/it, loss=5.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 5 Results ---\n",
      "Loss: 6.0075\n",
      "ROUGE-1: 0.5946 | ROUGE-2: 0.5653\n",
      "Prompt: 'i am going to' -> Gen: 'i am going to end something it r sn'\n",
      "Prompt: 'today is a' -> Gen: 'today is a bugsucquot uber'\n",
      "Prompt: 'this movie was' -> Gen: 'this movie was serious of the morning night'\n",
      "------------------------------\n",
      "Готово!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lstm_model import LSTMAutocompleteModel\n",
    "from lstm_train import train_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "from lstm_model import LSTMAutocompleteModel\n",
    "from lstm_train import train_model\n",
    "\n",
    "\n",
    "\n",
    "model = LSTMAutocompleteModel(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    embedding_dim=128, \n",
    "    hidden_dim=256, \n",
    "    n_layers=1  # 1 слой на CPU — это закон, иначе не успеем!\n",
    ")\n",
    "\n",
    "# Запускаем 5 эпох. Следи за Loss - он должен уверенно падать.\n",
    "trained_model = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    tokenizer, \n",
    "    epochs=5, \n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(trained_model.state_dict(), 'models/lstm_10k_final.pth')\n",
    "print(\"Готово!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96be73",
   "metadata": {},
   "source": [
    "Ниже приведено обучение модели на GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import os\n",
    "from lstm_model import LSTMAutocompleteModel\n",
    "from lstm_train import train_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- ОПТИМИЗИРОВАННЫЕ ПАРАМЕТРЫ (БЕЗОПАСНО ДЛЯ ПАМЯТИ) ---\n",
    "BATCH_SIZE = 16   # Сильно уменьшили, чтобы не забивать VRAM (память видеокарты)\n",
    "HIDDEN_DIM = 128  # Вернули к 128 — это значительно снизит потребление памяти при n_layers=2\n",
    "EMBED_DIM = 64    # Облегчили эмбеддинги\n",
    "EPOCHS = 10       # Раз у нас всего 500 строк, поставим 10 эпох для \"зазубривания\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Используем устройство: {DEVICE}\")\n",
    "\n",
    "\n",
    "\n",
    "# Инициализируем модель (2 слоя при скрытом слое 128 — хороший баланс)\n",
    "model = LSTMAutocompleteModel(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    embedding_dim=EMBED_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    n_layers=2\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Запускаем обучение\n",
    "trained_model = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    tokenizer, \n",
    "    epochs=EPOCHS, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Сохраняем финальные веса\n",
    "os.makedirs('models', exist_ok=True)\n",
    "torch.save(trained_model.state_dict(), 'models/lstm_final.pth')\n",
    "print(\"Обучение завершено, модель сохранена!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aeaed",
   "metadata": {},
   "source": [
    "## Сравнение моделей LSTM и Трансформера на тестовой выборке по метрикам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89857d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запуск оценки на всей тестовой выборке (159681 примеров)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка Трансформера distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ИТОГОВЫЕ МЕТРИКИ (FULL TEST)\n",
      "==================================================\n",
      "Модель      | ROUGE-1 | ROUGE-2\n",
      "------------|---------|---------\n",
      "LSTM        | 0.6138  | 0.5874\n",
      "DistilGPT2  | 0.6980  | 0.6544\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from eval_transformer_pipeline import evaluate_transformer_rouge\n",
    "from eval_lstm import evaluate_model_rouge\n",
    "from next_token_dataset import NextTokenDataset\n",
    "\n",
    "# 1. Загружаем полный тест\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_texts = test_df.iloc[:, 0].tolist()\n",
    "\n",
    "print(f\"Запуск оценки на всей тестовой выборке ({len(test_texts)} примеров)...\")\n",
    "\n",
    "# --- ЗАМЕР DISTILGPT2 ---\n",
    "# На CPU может занять 5-10 минут для полного теста, но это даст самые точные цифры\n",
    "tr_r1, tr_r2, _ = evaluate_transformer_rouge(\n",
    "    model_name=\"distilgpt2\", \n",
    "    test_texts=test_texts, \n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# --- ЗАМЕР LSTM ---\n",
    "# Создаем датасет для всего теста\n",
    "test_ds = NextTokenDataset('data/test.csv', tokenizer, max_length=64)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "r1_lstm, r2_lstm = evaluate_model_rouge(trained_model, test_loader, tokenizer, device=\"cpu\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ИТОГОВЫЕ МЕТРИКИ (FULL TEST)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Модель      | ROUGE-1 | ROUGE-2\")\n",
    "print(f\"------------|---------|---------\")\n",
    "print(f\"LSTM        | {r1_lstm:.4f}  | {r2_lstm:.4f}\")\n",
    "print(f\"DistilGPT2  | {tr_r1:.4f}  | {tr_r2:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea569220",
   "metadata": {},
   "source": [
    "## Сравнение на первых 10 примерах тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b2318b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "СРАВНЕНИЕ ГЕНЕРАЦИИ (ТОП-10 ПРИМЕРОВ):\n",
      "\n",
      "Пример №1\n",
      "Начало:  \"me either...\"\n",
      "Оригинал: \"me either\"\n",
      "LSTM:     [+ to]\n",
      "GPT2:     [+on]\n",
      "------------------------------\n",
      "Пример №2\n",
      "Начало:  \"why does the best...\"\n",
      "Оригинал: \"why does the best chicken in town have to have a line literally out the door for which i am willing to wait at pm on a sunday night\"\n",
      "LSTM:     [+ of]\n",
      "GPT2:     [+thing]\n",
      "------------------------------\n",
      "Пример №3\n",
      "Начало:  \"just woke up wavos...\"\n",
      "Оригинал: \"just woke up wavos soon havent been there in ageeees\"\n",
      "LSTM:     [+ and]\n",
      "GPT2:     [+after]\n",
      "------------------------------\n",
      "Пример №4\n",
      "Начало:  \"lovely day...\"\n",
      "Оригинал: \"lovely day\"\n",
      "LSTM:     [+ i]\n",
      "GPT2:     [+one]\n",
      "------------------------------\n",
      "Пример №5\n",
      "Начало:  \"work on...\"\n",
      "Оригинал: \"work on a sunday\"\n",
      "LSTM:     [+ the]\n",
      "GPT2:     [+the]\n",
      "------------------------------\n",
      "Пример №6\n",
      "Начало:  \"passed the testgetting my...\"\n",
      "Оригинал: \"passed the testgetting my motorcycle license\"\n",
      "LSTM:     [+ phone]\n",
      "GPT2:     [+car]\n",
      "------------------------------\n",
      "Пример №7\n",
      "Начало:  \"morning my oh my...\"\n",
      "Оригинал: \"morning my oh my its a wet onesunny southport disappeared overnight and was replaced with soggy southport\"\n",
      "LSTM:     [+ phone]\n",
      "GPT2:     [+oh]\n",
      "------------------------------\n",
      "Пример №8\n",
      "Начало:  \"lol yeah im thinkin...\"\n",
      "Оригинал: \"lol yeah im thinkin i should just need to go hydrate some first its bloody hawt today\"\n",
      "LSTM:     [+ the]\n",
      "GPT2:     [+can]\n",
      "------------------------------\n",
      "Пример №9\n",
      "Начало:  \"do you know who...\"\n",
      "Оригинал: \"do you know who quottheyquot are\"\n",
      "LSTM:     [+ i]\n",
      "GPT2:     [+you]\n",
      "------------------------------\n",
      "Пример №10\n",
      "Начало:  \"i spent hours all...\"\n",
      "Оригинал: \"i spent hours all night telling selena about my love life starting from february im a mess im sorry for keeping you up selena\"\n",
      "LSTM:     [+ the]\n",
      "GPT2:     [+day]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Инициализируем пайплайн для GPT2 один раз\n",
    "gpt2_pipe = pipeline(\"text-generation\", model=\"distilgpt2\", device=-1) # -1 это CPU\n",
    "\n",
    "print(\"\\nСРАВНЕНИЕ ГЕНЕРАЦИИ (ТОП-10 ПРИМЕРОВ):\\n\")\n",
    "\n",
    "for i, original_text in enumerate(test_texts[:10]):\n",
    "    # Берем первые 3-4 слова как \"затравку\" (prompt)\n",
    "    words = original_text.split()\n",
    "    prompt = \" \".join(words[:4]) if len(words) > 4 else \" \".join(words[:2])\n",
    "    \n",
    "    # --- Предсказание LSTM ---\n",
    "    trained_model.eval()\n",
    "    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(\"cpu\")\n",
    "    with torch.no_grad():\n",
    "        output = trained_model(input_ids)\n",
    "        # Мы берем [0], так как модель вернула (logits, hidden_states)\n",
    "        next_token_id = output[0][0, -1, :].argmax().item()\n",
    "        lstm_completion = tokenizer.decode([next_token_id])\n",
    "    \n",
    "    # --- Предсказание DistilGPT2 ---\n",
    "    gpt2_res = gpt2_pipe(prompt, max_new_tokens=1, num_return_sequences=1, pad_token_id=50256)\n",
    "    gpt2_completion = gpt2_res[0]['generated_text'].replace(prompt, \"\").strip()\n",
    "\n",
    "    print(f\"Пример №{i+1}\")\n",
    "    print(f\"Начало:  \\\"{prompt}...\\\"\")\n",
    "    print(f\"Оригинал: \\\"{original_text}\\\"\")\n",
    "    print(f\"LSTM:     [+{lstm_completion}]\")\n",
    "    print(f\"GPT2:     [+{gpt2_completion}]\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c07842a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "АНАЛИТИЧЕСКИЙ ОТЧЕТ ПО ИТОГАМ ЭКСПЕРИМЕНТА\n",
      "==================================================\n",
      "           Метрика  LSTM (Наш код)  DistilGPT2 (Transformer)\n",
      "   ROUGE-1 (Слова)        0.613770                  0.698022\n",
      "ROUGE-2 (Биграммы)        0.587376                  0.654399\n",
      "--------------------------------------------------\n",
      "ПОБЕДИТЕЛЬ: DistilGPT2\n",
      "ПРИЧИНА: Трансформер показал значительное превосходство в метриках. Благодаря механизму Self-Attention он лучше улавливает структуру предложений и генерирует более естественные окончания текстов.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_final_report(r1_lstm, r2_lstm, tr_r1, tr_r2):\n",
    "    print(\"=\"*50)\n",
    "    print(\"АНАЛИТИЧЕСКИЙ ОТЧЕТ ПО ИТОГАМ ЭКСПЕРИМЕНТА\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Сравнительная таблица\n",
    "    df_results = pd.DataFrame({\n",
    "        'Метрика': ['ROUGE-1 (Слова)', 'ROUGE-2 (Биграммы)'],\n",
    "        'LSTM (Наш код)': [r1_lstm, r2_lstm],\n",
    "        'DistilGPT2 (Transformer)': [tr_r1, tr_r2]\n",
    "    })\n",
    "    print(df_results.to_string(index=False))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Логика автоматического вывода\n",
    "    diff_r1 = tr_r1 - r1_lstm\n",
    "    \n",
    "    if diff_r1 > 0.05:\n",
    "        verdict = (\n",
    "            \"ПОБЕДИТЕЛЬ: DistilGPT2\\n\"\n",
    "            \"ПРИЧИНА: Трансформер показал значительное превосходство в метриках. \"\n",
    "            \"Благодаря механизму Self-Attention он лучше улавливает структуру предложений \"\n",
    "            \"и генерирует более естественные окончания текстов.\"\n",
    "        )\n",
    "    else:\n",
    "        verdict = (\n",
    "            \"ПОБЕДИТЕЛЬ: LSTM (с учетом ресурсов)\\n\"\n",
    "            \"ПРИЧИНА: Разница в качестве незначительна. LSTM эффективнее \"\n",
    "            \"адаптировалась к специфике датасета при гораздо меньших затратах \"\n",
    "            \"вычислительных ресурсов. Это оптимальный выбор для данной задачи.\"\n",
    "        )\n",
    "    \n",
    "    print(verdict)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Запуск (используй свои переменные с метриками)\n",
    "generate_final_report(r1_lstm, r2_lstm, tr_r1, tr_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4463a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
